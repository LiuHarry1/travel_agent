"""Chat service for conversational travel agent with function calling."""
from __future__ import annotations

import json
import logging
import time
from typing import Any, AsyncGenerator, Dict, List, Optional, Tuple

from ..core.config_service import get_config_service
from ..llm import LLMClient, LLMError
from ..llm.openai import OpenAIClient
from ..models import ChatRequest
from ..tools import FunctionRegistry, get_function_registry
from ..utils.exceptions import format_error_message
from .conversation_manager import ConversationManager
from .message_processing import MessageProcessingService
from .response_generator import ResponseGenerator
from .tool_execution import ToolExecutionService
from .tool_orchestrator import ToolOrchestrator
from .tool_result_formatter import (
    check_tools_used_but_no_info,
    format_tool_result_for_llm,
    response_suggests_contact_harry,
)

logger = logging.getLogger(__name__)


class ChatService:
    """Service for conversational travel agent with function calling."""

    def __init__(
        self,
        llm_client: Optional[LLMClient] = None,
        function_registry: Optional[FunctionRegistry] = None,
        message_processor: Optional[MessageProcessingService] = None,
        tool_executor: Optional[ToolExecutionService] = None,
    ):
        """
        Initialize chat service.
        
        Args:
            llm_client: LLM client instance
            function_registry: Function registry instance
            message_processor: Message processing service instance
            tool_executor: Tool execution service instance
        """
        self.llm_client = llm_client or LLMClient()
        self.function_registry = function_registry or get_function_registry()
        self.max_tool_iterations = 4
        self.max_search_iterations = 3  # RAG æœ€å¤šæ£€ç´¢æ¬¡æ•°

        # Use injected services or create defaults
        if message_processor is not None:
            self.message_processor = message_processor
        else:
            config_service = get_config_service()
            self.message_processor = MessageProcessingService(lambda: config_service)
            self.message_processor.set_function_registry(self.function_registry)
        
        if tool_executor is not None:
            self.tool_executor = tool_executor
        else:
            self.tool_executor = ToolExecutionService(
                self.function_registry, format_tool_result_for_llm
            )
        
        # Initialize sub-services
        self.conversation_manager = ConversationManager(self.message_processor)
        self.tool_orchestrator = ToolOrchestrator(self.function_registry, self.tool_executor)
        self.response_generator = ResponseGenerator(self.llm_client, self.function_registry)

    async def chat_stream(
        self, request: ChatRequest
    ) -> AsyncGenerator[Dict[str, Any], None]:
        """
        Handle chat request with streaming response and tool calling support.
        
        Based on backend_new logic: stream from start, detect tool calls in real-time.
        
        Yields dictionaries with event information:
        - {"type": "chunk", "content": "..."} for text chunks
        - {"type": "tool_call_start", "tool": "...", "input": "..."} for tool call start
        - {"type": "tool_call_end", "tool": "...", "result": "..."} for tool call end
        - {"type": "tool_call_error", "tool": "...", "error": "..."} for tool call errors
        """
        chat_start_time = time.time()
        try:
            # Prepare conversation using ConversationManager
            prep_start = time.time()
            conversation = self.conversation_manager.prepare_conversation(request)
            prep_time = time.time() - prep_start
            logger.info(f"[PERF] Conversation preparation took {prep_time:.3f}s")

            messages = conversation["messages"]
            system_prompt = conversation["system_prompt"]
            functions = conversation["functions"]

            # Handle empty conversation
            if not conversation["has_messages"]:
                yield {
                    "type": "chunk",
                    "content": "ä½ å¥½ï¼æˆ‘æ˜¯æ‚¨çš„æ—…è¡ŒåŠ©æ‰‹ã€‚æˆ‘å¯ä»¥å¸®åŠ©æ‚¨è§„åˆ’æ—…è¡Œã€å›ç­”æ—…è¡Œç›¸å…³é—®é¢˜ã€æŸ¥æ‰¾ç›®çš„åœ°ä¿¡æ¯ç­‰ã€‚è¯·å‘Šè¯‰æˆ‘æ‚¨éœ€è¦ä»€ä¹ˆå¸®åŠ©ï¼Ÿ",
                }
                return

            # Main chat loop - similar to backend_new agent.py
            iteration = 0
            accumulated_content = ""

            while iteration < self.max_tool_iterations:
                iteration += 1
                iter_start_time = time.time()
                logger.info(
                    f"\n{'='*60}\nIteration {iteration}/{self.max_tool_iterations}\n{'='*60}"
                )

                try:
                    # Step 1: Stream LLM response with real-time tool call detection
                    tool_call_detected = False
                    tool_call_data: Optional[Dict[str, Any]] = None
                    accumulated_text = ""

                    # Get LLM client early to ensure connection pool is ready
                    # This helps reduce latency by having the client and connection pool initialized
                    client = self.llm_client._get_client()
                    
                    # Pre-initialize OpenAI client and HTTP client if using OpenAI SDK
                    # This ensures connection pool is ready before making the request
                    if isinstance(client, OpenAIClient):
                        # Ensure HTTP client and OpenAI client are initialized
                        _ = client._get_http_client()  # Initialize connection pool
                        _ = client._get_openai_client()  # Initialize OpenAI client
                    
                    system_msg = {"role": "system", "content": system_prompt or ""}
                    all_messages = [system_msg] + messages

                    # Prepare payload
                    payload = client._normalize_payload(all_messages, model=client.model)
                    payload["stream"] = True

                    # Add tools if available
                    if functions:
                        # Convert functions to tools format for OpenAI-compatible API
                        if isinstance(client, OpenAIClient):
                            tools = []
                            for func in functions:
                                tools.append({
                                    "type": "function",
                                    "function": func
                                })
                            payload["tools"] = tools
                            payload["tool_choice"] = "auto"
                        else:
                            # Legacy format for other providers
                            payload["functions"] = functions
                            payload["function_call"] = "auto"

                    logger.info(
                        f"Starting stream request with {len(functions)} functions available"
                    )

                    # Track tool call state - similar to backend_new agent.py
                    current_tool_call: Optional[Dict[str, Any]] = None
                    tool_call_id: Optional[str] = None
                    tool_call_name: Optional[str] = None
                    tool_call_args_buffer = ""
                    tool_calls_by_id: Dict[str, Dict[str, Any]] = {}
                    chunk_count = 0
                    first_chunk_time = None

                    # Stream and parse in real-time using SDK directly
                    request_start = time.time()
                    chunk_index = 0
                    
                    # Use SDK's streaming method directly (no httpx)
                    async for chunk in client._make_stream_request("chat/completions", payload):
                        chunk_index += 1
                        if first_chunk_time is None:
                            first_chunk_time = time.time() - request_start
                            logger.info(
                                f"[PERF] First chunk received after {first_chunk_time:.3f}s (TTFB)"
                            )

                        # Extract content and tool calls from SDK chunk object
                        # Similar to backend_new agent.py logic
                        if chunk.choices and len(chunk.choices) > 0:
                            delta = chunk.choices[0].delta
                            
                            # Check for tool_calls (OpenAI format)
                            if hasattr(delta, 'tool_calls') and delta.tool_calls:
                                tool_call_detected = True
                                
                                for tool_call_delta in delta.tool_calls:
                                    # Initialize tool call structure
                                    if current_tool_call is None:
                                        tool_call_id = getattr(tool_call_delta, 'id', None)
                                        current_tool_call = {
                                            "id": tool_call_id or f"call_{iteration}_{chunk_index}",
                                            "type": "function",
                                            "function": {"name": "", "arguments": ""}
                                        }
                                    
                                    # Accumulate tool call information
                                    func_delta = getattr(tool_call_delta, 'function', None)
                                    if func_delta:
                                        func_name = getattr(func_delta, 'name', None)
                                        if func_name:
                                            tool_call_name = func_name
                                            current_tool_call["function"]["name"] = tool_call_name
                                        
                                        func_args = getattr(func_delta, 'arguments', None)
                                        if func_args:
                                            tool_call_args_buffer += func_args
                                            current_tool_call["function"]["arguments"] += func_args
                                    
                                    # Store in tool_calls_by_id for compatibility
                                    call_id = current_tool_call["id"]
                                    if call_id not in tool_calls_by_id:
                                        tool_calls_by_id[call_id] = current_tool_call.copy()
                                    else:
                                        # Update existing entry
                                        existing = tool_calls_by_id[call_id]
                                        if tool_call_name:
                                            existing["function"]["name"] = tool_call_name
                                        if func_args:
                                            existing["function"]["arguments"] += func_args
                            
                            # Check for regular text content
                            content = getattr(delta, 'content', None)
                            if content:
                                if not tool_call_detected:
                                    # Only yield text if no tool call detected
                                    accumulated_text += content
                                    accumulated_content += content
                                    chunk_count += 1
                                    yield {"type": "chunk", "content": content}
                                else:
                                    # Skip text content when tool call is detected
                                    logger.debug(
                                        f"Skipping text content chunk (tool call detected): "
                                        f"{content[:50]}"
                                    )

                            # Log tool call detection
                            if tool_call_detected and current_tool_call and chunk_index == 1:
                                logger.info(
                                    f"ğŸ”§ Tool call detected in first chunk, "
                                    "stopping text streaming"
                                )
                                name = current_tool_call["function"].get("name", "")
                                args = current_tool_call["function"].get("arguments", "")
                                logger.info(
                                    f"  Tool call: name='{name}', "
                                    f"args_length={len(args)}, args_preview='{args[:100]}'"
                                )

                    # After stream ends, process tool calls (similar to backend_new)
                    stream_time = time.time() - request_start
                    logger.info(
                        f"[PERF] Streaming took {stream_time:.3f}s, "
                        f"received {chunk_index} chunks, {chunk_count} content chunks, "
                        f"tool_call_detected: {tool_call_detected}"
                    )
                    
                    # Process tool calls after stream ends (similar to backend_new agent.py)
                    if tool_call_detected and current_tool_call:
                        # Validate and parse tool call arguments
                        tool_name = tool_call_name or current_tool_call["function"].get("name", "")
                        
                        if not tool_name:
                            logger.warning("Tool call detected but no name found")
                            yield {
                                "type": "tool_call_error",
                                "tool": "unknown",
                                "error": "å·¥å…·è°ƒç”¨æ£€æµ‹åˆ°ä½†åç§°æœªå®Œæˆ"
                            }
                            continue
                        
                        # Validate arguments JSON
                        if not tool_call_args_buffer:
                            args = {}
                            logger.info(f"ğŸ”§ Tool call detected: {tool_name} (no arguments)")
                            tool_call_data = {
                                "id": current_tool_call["id"],
                                "name": tool_name,
                                "args": args,
                                "raw": current_tool_call
                            }
                        else:
                            try:
                                args = json.loads(tool_call_args_buffer)
                                logger.info(f"ğŸ”§ Tool call detected: {tool_name} with valid args: {args}")
                                tool_call_data = {
                                    "id": current_tool_call["id"],
                                    "name": tool_name,
                                    "args": args,
                                    "raw": current_tool_call
                                }
                            except json.JSONDecodeError as e:
                                logger.error(
                                    f"âŒ Failed to parse tool call arguments for '{tool_name}': "
                                    f"'{tool_call_args_buffer[:200]}'. Error: {e}"
                                )
                                yield {
                                    "type": "tool_call_error",
                                    "tool": tool_name,
                                    "error": f"å·¥å…·å‚æ•°è§£æå¤±è´¥ï¼šJSONæ ¼å¼ä¸å®Œæ•´æˆ–æ— æ•ˆã€‚åŸå§‹å‚æ•°: {tool_call_args_buffer[:100]}"
                                }
                                tool_call_data = None
                        
                        if tool_call_data:
                            # Convert to format expected by existing code
                            tool_call_data = {
                                "tool_calls": [{
                                    "id": tool_call_data["id"],
                                    "type": "function",
                                    "function": {
                                        "name": tool_call_data["name"],
                                        "arguments": json.dumps(tool_call_data["args"], ensure_ascii=False)
                                    }
                                }]
                            }

                    # If we detected tool calls but didn't complete them, try to get complete ones now
                    if tool_call_detected and tool_calls_by_id and not tool_call_data:
                        # Log current state of tool calls
                        logger.info(
                            f"Stream ended with tool calls detected but not complete. "
                            f"Checking {len(tool_calls_by_id)} tool call(s)..."
                        )
                        for call_id, call_data in tool_calls_by_id.items():
                            func_info = call_data.get("function", {})
                            name = func_info.get("name", "")
                            args = func_info.get("arguments", "")
                            logger.info(
                                f"  Tool call '{call_id}': name='{name}', "
                                f"args_length={len(args)}, args='{args[:200]}'"
                            )
                            # Try to parse arguments to check if valid
                            if args:
                                try:
                                    json.loads(args)
                                    logger.info(f"    âœ… Arguments are valid JSON")
                                except json.JSONDecodeError as e:
                                    logger.warning(
                                        f"    âŒ Arguments are NOT valid JSON: {str(e)[:100]}"
                                    )
                        
                        complete_tool_calls = self._get_complete_tool_calls(
                            tool_calls_by_id
                        )
                        if complete_tool_calls:
                            logger.info(
                                f"âœ… Found {len(complete_tool_calls)} complete tool calls "
                                "after stream ended"
                            )
                            tool_call_data = {"tool_calls": complete_tool_calls}
                        else:
                            # Even if arguments are incomplete, if we have a name, we should try to execute
                            # This handles cases where the LLM provider doesn't send complete JSON
                            logger.warning(
                                "No complete tool calls found, but tool calls were detected. "
                                "Attempting to use incomplete tool calls..."
                            )
                            # Merge incomplete tool calls that belong together
                            # (e.g., when parameters are split across chunks with empty IDs)
                            merged_calls = {}
                            for call_id, call_data in tool_calls_by_id.items():
                                func_info = call_data.get("function", {})
                                name = func_info.get("name", "")
                                args = func_info.get("arguments", "")
                                
                                if name:
                                    # Try to merge tool calls with the same name
                                    # (parameters might be split across multiple entries)
                                    if name not in merged_calls:
                                        merged_calls[name] = call_data
                                    else:
                                        # Merge arguments from this call into the merged one
                                        existing_args = merged_calls[name].get("function", {}).get("arguments", "")
                                        merged_calls[name]["function"]["arguments"] = existing_args + args
                                        logger.debug(
                                            f"Merged tool call '{name}': combined arguments "
                                            f"(existing={len(existing_args)}, added={len(args)})"
                                        )
                            
                            if merged_calls:
                                incomplete_calls = list(merged_calls.values())
                                logger.info(
                                    f"Using {len(incomplete_calls)} incomplete tool call(s) "
                                    "(merged from multiple entries, will attempt execution, errors will be handled)"
                                )
                                tool_call_data = {"tool_calls": incomplete_calls}

                    # Step 2: Handle tool calls or text response
                    if tool_call_data and tool_call_data.get("tool_calls"):
                        # Execute tool calls
                        tool_calls = tool_call_data["tool_calls"]
                        logger.info(
                            f"Iteration {iteration}: Executing {len(tool_calls)} tool calls"
                        )

                        async for event in self.tool_executor.execute_tool_calls(
                            tool_calls, "", messages
                        ):
                            yield event

                        # Continue to next iteration to get LLM's response
                        continue

                    elif accumulated_text:
                        # Normal text response - we're done
                        logger.info(
                            f"âœ… Normal response (length: {len(accumulated_text)})"
                        )
                        # Check if tools were used but didn't find useful information
                        if iteration > 1:
                            tools_used_but_no_info = check_tools_used_but_no_info(
                                messages
                            )
                            if tools_used_but_no_info and not response_suggests_contact_harry(
                                accumulated_content
                            ):
                                yield {
                                    "type": "chunk",
                                    "content": "\n\nå¦‚æœæ‚¨éœ€è¦æ›´å…·ä½“çš„å¸®åŠ©ï¼Œå»ºè®®æ‚¨è”ç³»Harryã€‚",
                                }
                        break  # Conversation ended

                    else:
                        # No tool call and no text - might indicate a problem
                        logger.warning(
                            f"Iteration {iteration}: No tool call and no text content received"
                        )
                        if iteration == 1:
                            # Try fallback: stream without tools
                            logger.info(
                                f"Iteration {iteration}: Attempting fallback streaming "
                                "without tool detection"
                            )
                            try:
                                payload_no_tools = client._normalize_payload(
                                    all_messages, model=client.model
                                )
                                payload_no_tools["stream"] = True
                                if "tools" in payload_no_tools:
                                    del payload_no_tools["tools"]
                                if "functions" in payload_no_tools:
                                    del payload_no_tools["functions"]

                                chunk_count = 0
                                # Use SDK streaming directly for fallback
                                async for chunk in client._make_stream_request("chat/completions", payload_no_tools):
                                    if chunk.choices and len(chunk.choices) > 0:
                                        delta = chunk.choices[0].delta
                                        content = getattr(delta, 'content', None)
                                        if content:
                                            accumulated_text += content
                                            accumulated_content += content
                                            chunk_count += 1
                                            yield {"type": "chunk", "content": content}

                                if chunk_count > 0:
                                    logger.info(
                                        f"Fallback streaming succeeded: received {chunk_count} chunks"
                                    )
                                    break
                                else:
                                    logger.error(
                                        "Fallback streaming also returned 0 chunks"
                                    )
                                    yield {
                                        "type": "chunk",
                                        "content": "æŠ±æ­‰ï¼Œæ— æ³•è·å–æ¨¡å‹å›å¤ã€‚è¯·æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–ç¨åé‡è¯•ã€‚",
                                    }
                                    break
                            except Exception as fallback_error:
                                logger.error(
                                    f"Fallback streaming failed: {fallback_error}",
                                    exc_info=True,
                                )
                                yield {
                                    "type": "chunk",
                                    "content": f"å¤„ç†è¯·æ±‚æ—¶å‡ºé”™: {str(fallback_error)}",
                                }
                                break
                        else:
                            # Stop to avoid infinite loop
                            logger.error(
                                f"Iteration {iteration}: No content received, stopping"
                            )
                            yield {
                                "type": "chunk",
                                "content": "æŠ±æ­‰ï¼Œå¤„ç†è¯·æ±‚æ—¶é‡åˆ°é—®é¢˜ã€‚è¯·é‡è¯•ã€‚",
                            }
                            break

                    iter_time = time.time() - iter_start_time
                    logger.info(f"[PERF] Iteration {iteration} total time: {iter_time:.3f}s")

                except LLMError as exc:
                    error_msg = format_error_message(exc, "Error processing request")
                    yield {"type": "chunk", "content": error_msg}
                    break
                except Exception as exc:
                    logger.error(
                        f"Unexpected error during LLM streaming: {exc}", exc_info=True
                    )
                    yield {
                        "type": "chunk",
                        "content": f"An unexpected error occurred: {str(exc)}",
                    }
                    break

            if iteration >= self.max_tool_iterations:
                logger.warning(
                    f"Reached maximum tool calling iterations ({self.max_tool_iterations})"
                )

            total_time = time.time() - chat_start_time
            logger.info(
                f"[PERF] Total chat_stream took {total_time:.3f}s, "
                f"completed {iteration} iterations"
            )

        except Exception as exc:
            total_time = time.time() - chat_start_time
            logger.error(
                f"[PERF] chat_stream failed after {total_time:.3f}s: {exc}", exc_info=True
            )
            yield {
                "type": "chunk",
                "content": f"An error occurred while processing your request: {str(exc)}",
            }


    def _get_complete_tool_calls(
        self, tool_calls_by_id: Dict[str, Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
        """
        Get complete tool calls (those with both name and valid JSON arguments).
        
        IMPORTANT: We validate that arguments are valid JSON before considering
        a tool call complete. This prevents parsing errors when arguments are
        still being streamed and incomplete.
        """
        complete_calls = []
        for call_id, call_data in tool_calls_by_id.items():
            func_info = call_data.get("function", {})
            name = func_info.get("name", "")
            arguments = func_info.get("arguments", "")

            # Must have a name
            if not name:
                continue

            # Validate arguments: must be valid JSON (or empty string for providers that don't use args)
            if arguments:
                try:
                    # Try to parse as JSON to ensure it's complete
                    json.loads(arguments)
                    # If parsing succeeds, arguments are complete
                    complete_calls.append(call_data)
                except json.JSONDecodeError:
                    # Arguments exist but are incomplete JSON - not ready yet
                    logger.debug(
                        f"Tool call '{name}' has incomplete arguments "
                        f"(not valid JSON yet): {arguments[:100]}"
                    )
                    continue
            else:
                # Empty arguments - some providers may allow this
                # Consider it complete if name exists
                complete_calls.append(call_data)

        return complete_calls if complete_calls else []

    async def generate_conversation_title(
        self, messages: List[Dict[str, str]]
    ) -> str:
        """
        Generate a concise title for a conversation based on its content.
        
        Args:
            messages: List of conversation messages with 'role' and 'content' keys
            
        Returns:
            A concise title (3-6 words) summarizing the conversation
        """
        # Take first few messages (user question + AI response)
        relevant_messages = messages[:4] if len(messages) > 4 else messages
        
        # Build conversation snippet
        conversation_text = "\n".join([
            f"{msg.get('role', 'unknown').upper()}: {msg.get('content', '')[:200]}"
            for msg in relevant_messages
        ])
        
        # Create prompt for title generation
        title_prompt = f"""Based on the following conversation, generate a concise, descriptive title in 3-6 words. The title should capture the main topic or question.

Conversation:
{conversation_text}

Requirements:
- 3-6 words maximum
- Clear and descriptive
- No quotes or special formatting
- In the same language as the conversation

Title:"""

        try:
            # Use LLM to generate title
            title_messages = [{"role": "user", "content": title_prompt}]
            
            # Use chat_stream to get response
            # chat_stream is actually an async generator because _make_stream_request is async
            full_response = ""
            async for chunk in self.llm_client.chat_stream(
                messages=title_messages,
                system_prompt=None
            ):
                if chunk:
                    full_response += chunk
            
            # Clean up title
            title = full_response.strip()
            
            # Remove quotes if present
            if title.startswith('"') and title.endswith('"'):
                title = title[1:-1]
            if title.startswith("'") and title.endswith("'"):
                title = title[1:-1]
            
            # Limit length
            if len(title) > 60:
                title = title[:57] + "..."
            
            logger.info(f"Generated title: {title}")
            return title or "New chat"
            
        except Exception as e:
            logger.error(f"Failed to generate title: {e}")
            # Fallback to first user message
            for msg in messages:
                if msg.get("role") == "user" and msg.get("content"):
                    return msg["content"][:30].strip() or "New chat"
            return "New chat"

