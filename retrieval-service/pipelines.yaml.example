# Pipelines Configuration Example
# This file defines pipeline-specific configurations for the retrieval service.
# Each pipeline can have its own Milvus connection, embedding models, rerank service, and LLM settings.

# Default pipeline name (used when pipeline_name is not specified in API calls)
default: memory

# Pipeline configurations
pipelines:
  # Example pipeline 1: Development environment
  memory:
    # Milvus vector database configuration
    milvus:
      host: localhost
      port: 19530
      user: ""  # Optional: leave empty if not using authentication
      password: ""  # Optional: leave empty if not using authentication
      database: default  # Database name in Milvus
      collection: memory_doc_db  # Default collection name (used if not specified in embedding_models)

    # Embedding models to use for retrieval
    # Format options:
    #   - Simple: "qwen" (uses default collection)
    #   - With model: "qwen:text-embedding-v2" (uses default collection)
    #   - With collection: "qwen:memory_doc_db" (qwen model -> memory_doc_db collection)
    #   - Full: "qwen:text-embedding-v2:memory_doc_db" (provider:model:collection)
    #   - Object format:
    #     - model: "qwen"
    #       collection: "memory_doc_db"
    # Available providers:
    #   - qwen: Qwen embedding models
    #   - bge-en: BAAI/bge-large-en-v1.5 (English, API: http://10.150.10.120:6000)
    #   - bge-zh: BAAI/bge-large-zh-v1.5 (Chinese, API: http://10.150.10.120:6001)
    #   - nemotron: nvidia/llama-nemotron-embed-1b-v2 (API: http://10.150.10.120:6002)
    #   - snowflake: Snowflake/snowflake-arctic-embed-l (API: http://10.150.10.120:6003)
    #   - bge: Generic BGE (uses default API URL)
    #   - openai: OpenAI embedding models
    embedding_models:
      - qwen  # Uses default collection: memory_doc_db
      # - bge-en  # BAAI/bge-large-en-v1.5 (English)
      # - bge-zh  # BAAI/bge-large-zh-v1.5 (Chinese)
      # - nemotron  # nvidia/llama-nemotron-embed-1b-v2
      # - snowflake  # Snowflake/snowflake-arctic-embed-l
      # - qwen:text-embedding-v2  # Explicit model name, uses default collection
      # - qwen:memory_doc_db  # qwen model -> memory_doc_db collection
      # - qwen:text-embedding-v2:memory_doc_db  # Full format
      # - model: "bge-en"
      #   collection: "bge_en_collection"  # Object format for different collection
      # - bge-en:BAAI/bge-large-en-v1.5:bge_collection  # BGE English model with specific collection
      # - bge-zh:BAAI/bge-large-zh-v1.5:bge_zh_collection  # BGE Chinese model with specific collection
      # - nemotron:nvidia/llama-nemotron-embed-1b-v2:nemotron_collection  # Nemotron model
      # - snowflake:Snowflake/snowflake-arctic-embed-l:snowflake_collection  # Snowflake model
      # - openai:text-embedding-3-small:openai_collection  # OpenAI model with specific collection

    # Rerank service configuration
    rerank:
      api_url: ""  # Rerank API endpoint (leave empty to use mock implementation)
      model: ""  # Optional: rerank model name
      timeout: 30  # Request timeout in seconds

    # LLM filter configuration (for filtering irrelevant chunks)
    llm_filter:
      # API key can use environment variable: env:VAR_NAME
      api_key: env:DASHSCOPE_API_KEY  # or provide directly: "sk-xxxxx"
      base_url: https://dashscope.aliyuncs.com/compatible-mode/v1
      model: qwen-plus

    # Retrieval pipeline parameters
    retrieval:
      top_k_per_model: 10  # Number of results per embedding model
      rerank_top_k: 20  # Number of results to keep after rerank
      final_top_k: 10  # Final number of results after LLM filtering

    # Chunk size limits for each stage
    chunk_sizes:
      initial_search: 100  # Max chunks from initial search per model
      rerank_input: 50  # Max chunks to send to rerank
      llm_filter_input: 20  # Max chunks to send to LLM filter

  # Example pipeline 2: Multi-model with different collections
  multi_model:
    milvus:
      host: localhost
      port: 19530
      user: ""
      password: ""
      database: default
      collection: default_collection  # Default collection

    # Each embedding model uses its own collection
    embedding_models:
      - qwen:qwen_collection  # qwen model -> qwen_collection
      - bge:bge_collection  # bge model -> bge_collection
      - openai:openai_collection  # openai model -> openai_collection

    rerank:
      api_url: http://rerank-service:8009/rerank
      model: bge-reranker-base
      timeout: 60

    llm_filter:
      api_key: env:DASHSCOPE_API_KEY
      base_url: https://dashscope.aliyuncs.com/compatible-mode/v1
      model: qwen-max

    retrieval:
      top_k_per_model: 15
      rerank_top_k: 30
      final_top_k: 15

    chunk_sizes:
      initial_search: 150
      rerank_input: 75
      llm_filter_input: 30

  # Example pipeline 3: Testing environment
  test_pipeline:
    milvus:
      host: localhost
      port: 19530
      user: ""
      password: ""
      database: default
      collection: test_collection

    embedding_models:
      - qwen  # Uses test_collection

    rerank:
      api_url: ""  # Use mock rerank for testing
      model: ""
      timeout: 30

    llm_filter:
      api_key: env:TEST_DASHSCOPE_API_KEY
      base_url: https://dashscope.aliyuncs.com/compatible-mode/v1
      model: qwen-plus

    retrieval:
      top_k_per_model: 5
      rerank_top_k: 10
      final_top_k: 5

    chunk_sizes:
      initial_search: 50
      rerank_input: 25
      llm_filter_input: 10


